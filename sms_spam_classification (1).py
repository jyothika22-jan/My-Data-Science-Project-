# -*- coding: utf-8 -*-
"""SMS spam classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wqjt3op9FlMre0w7_SAzCqWK2GepWg42
"""

!pip install --upgrade nltk
!pip install --upgrade scikit-learn

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)


import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import re
import nltk

from matplotlib.colors import ListedColormap

from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer

from sklearn import metrics
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

nltk.download('punkt')  # Downloads the 'punkt' resource which is often needed
nltk.download('wordnet')  # Downloads the 'wordnet' resource
nltk.download('averaged_perceptron_tagger')  # Downloads the 'averaged_perceptron_tagger' resource
# nltk.download('all')

#Loading data
df = pd.read_csv("/content/spam.csv", encoding='latin1')

df.info()

df

# Dropping the unused collumns
drop_col_list = ["Unnamed: 2","Unnamed: 3","Unnamed: 4"]
df = df.drop(df[drop_col_list], axis=1)

# Renaming the columns for better readability
df.rename(columns = {"v1":"Target", "v2":"Text"}, inplace = True)
df.head()

# Group by 'Target' and count the occurrences
target_counts = df.groupby('Target').size().reset_index(name='Count')
print(target_counts)

# Plotting the target and check if the data is imbalanced or not
plt.figure(figsize=(12,8))
fig = sns.countplot(x= df["Target"], palette=['green','purple'])
fig.set_title("Number of Spam and Ham")
fig.set_xlabel("Classes")
fig.set_ylabel("Number of Data points")
plt.show(fig)

# Adding more columns to show the number of characters, words, and sentences with NLTK library
df["No_of_Characters"] = df["Text"].apply(len)
df["No_of_Words"] = df.apply(lambda row: nltk.word_tokenize(row["Text"]), axis=1).apply(len)
df["No_of_Sentences"] = df.apply(lambda row: nltk.sent_tokenize(row["Text"]), axis=1).apply(len)

df.describe().T

df

plt.figure(figsize=(12,8))
fig = sns.pairplot(data=df, hue="Target",palette=['purple','yellow'])
plt.suptitle("The relationship between no. characters, words, and sentences")
plt.tight_layout()
plt.show(fig)

# Dropping the outliers.
df = df[(df["No_of_Characters"] < 500)]
df.shape

plt.figure(figsize=(12,8))
fig = sns.pairplot(data=df, hue="Target",palette=['purple','yellow'])
plt.suptitle("The relationship between no. characters, words, and sentences")
plt.tight_layout()
plt.show(fig)

df.head()

def clean_text(text):
    clean = re.sub('[^a-zA-Z]', ' ', text) # Replacing all non-alphabetic characters with a space
    clean = clean.lower() # converting to lowecase
    clean = clean.split() # splits the cleaned text sms into a list of words
    clean = ' '.join(clean) # joins the list of words back into a string, using a single space as the separator between the words
    return clean

df.copy()
df.loc[:, "Clean_Text"] = df["Text"].apply(clean_text)

df

# The nltk.word_tokenize function from the Natural Language Toolkit (NLTK) is used
# to tokenize the text in the "Clean_Text" column of that row
df.loc[:, "Tokenized_Text"] = df.apply(lambda row: nltk.word_tokenize(row["Clean_Text"]), axis=1)

df.head()

import nltk
nltk.download('stopwords')

def remove_stopwords(Tokenized_Text):
   stop_words = set(stopwords.words("english"))
   filtered_text = [word for word in Tokenized_Text if word not in stop_words]
   return filtered_text

df["No_Stopword_Text"] = df["Tokenized_Text"].apply(remove_stopwords)

df[['Tokenized_Text','No_Stopword_Text']].head()

lemmatizer = WordNetLemmatizer()

def lemmatize_word(text):
    lemmas = [lemmatizer.lemmatize(word) for word in text]
    return lemmas

df.loc[:, "Lemmatized_Text"] = df["No_Stopword_Text"].apply(lemmatize_word)
df[['No_Stopword_Text', 'Lemmatized_Text']]

stemmer = nltk.SnowballStemmer("english")

def stemm_text(text):
    text = [stemmer.stem(word) for word in text]
    return text

df.loc[:, 'Stemmed_Text'] = df['No_Stopword_Text'].apply(stemm_text).copy()
df[['No_Stopword_Text', 'Stemmed_Text']]

df.loc[:, 'LS_Text'] = df['Lemmatized_Text'].apply(stemm_text).copy()
df[['Lemmatized_Text', 'LS_Text']]

# Creating a corpus of text feature to encode further into vectorized form
corpus= []
for i in df["LS_Text"]:
    msg = ' '.join([row for row in i]) # Joins the strings within the list into a single space-separated string
    corpus.append(msg) # Appends the processed string (msg) to the corpus list

corpus[:5]

tfidf = TfidfVectorizer()
X = tfidf.fit_transform(corpus).toarray()

label_encoder = LabelEncoder()
df.loc[:, "Target"] = label_encoder.fit_transform(df["Target"])

# Splitting the dataset into testing and training sets
y = df["Target"].astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

!pip install catboost

from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

classifiers = [
    CatBoostClassifier(verbose=False),
    LogisticRegression(random_state=42),
    RandomForestClassifier(),
    SVC(),
    MultinomialNB(),
    KNeighborsClassifier()
]

for cls in classifiers:
    cls.fit(X_train, y_train)

# Cross Validation Score
for model in classifiers:
    cv_scores = cross_val_score(model, X_train, y_train, scoring="accuracy", cv=10)
    print(model.__class__.__name__, ":", cv_scores.mean())

from sklearn import metrics

classifiers = [
    CatBoostClassifier(verbose=False),
    LogisticRegression(random_state=42),
    RandomForestClassifier(),
    SVC(),
    MultinomialNB(),
    KNeighborsClassifier()
]

precision = []
recall = []
f1_score = []
train_set_acc = []
test_set_acc = []

for clf in classifiers:
    clf.fit(X_train, y_train)
    pred_train = clf.predict(X_train)
    pred_test = clf.predict(X_test)
    prec = metrics.precision_score(y_test, pred_test)
    recal = metrics.recall_score(y_test, pred_test)
    f1_s = metrics.f1_score(y_test, pred_test)
    train_acc = clf.score(X_train, y_train)
    test_acc = clf.score(X_test, y_test)

    precision.append(prec)
    recall.append(recal)
    f1_score.append(f1_s)
    train_set_acc.append(train_acc)
    test_set_acc.append(test_acc)

eval_data = {'Precision':precision,
             'Recall':recall,
             'F1_Score':f1_score,
             'Train_Accuracy':train_set_acc,
             'Test_Accuracy':test_set_acc}

# Create pandas DataFrame with classifier names as indices.
classifier_model = [cls.__class__.__name__ for cls in classifiers]
result_df = pd.DataFrame(eval_data, index=classifier_model)
cmap = ListedColormap(["purple","violet"])
result_df.style.background_gradient(cmap=cmap)

# Create new columns for each criterion
result_df['Precision_Rank'] = result_df['Precision'].rank(ascending=False)
result_df['Recall_Rank'] = result_df['Recall'].rank(ascending=False)
result_df['F1_Score_Rank'] = result_df['F1_Score'].rank(ascending=False)
result_df['Train_Accuracy_Rank'] = result_df['Train_Accuracy'].rank(ascending=False)
result_df['Test_Accuracy_Rank'] = result_df['Test_Accuracy'].rank(ascending=False)

# Calculate the mean of the ranks to create an overall rank
result_df['Overall_Rank'] = result_df[['Precision_Rank', 'Recall_Rank',
                                       'F1_Score_Rank', 'Train_Accuracy_Rank',
                                       'Test_Accuracy_Rank']].mean(axis=1)
result_df

fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 10))

for cls, ax in zip(classifiers, axes.flatten()):
    y_pred = cls.predict(X_test)
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", ax=ax)
    ax.set_title(type(cls).__name__)

plt.tight_layout()
plt.show()

predictions_results = []

for i, model in enumerate(classifiers):
    correct_train = (pred_train == y_train).tolist()
    correct_test = (pred_test == y_test).tolist()

    predictions_results.append({
        'Model': model.__class__.__name__,
        'Train_Predictions': pred_train,
        'Train_Correct': correct_train,
        'Test_Predictions': pred_test,
        'Test_Correct': correct_test
    })

predictions_df = pd.DataFrame(predictions_results)

predictions_df

# Filter the DataFrame for best model
best_results = predictions_df[predictions_df['Model'] == result_df['Overall_Rank'].idxmax()].iloc[0]

best_df = pd.DataFrame({
    'Actual_Test': y_test.tolist(),
    'Test_Predictions': best_results['Test_Predictions'],
    'Test_Result': best_results['Test_Correct']
})

print(best_df)

incorrect_predictions_df = best_df[best_df['Test_Result'] == False]
incorrect_predictions_df

